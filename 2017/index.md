---
title: Workshop on Meta-Learning (MetaLearn 2017)
description: "@NIPS 2017 <br> Saturday 09 December 2017 <br> Hyatt Regency Long Beach, Beacon Ballroom D+E+F+H"
permalink: /2017/index.html
weight: -1
redirect_from: "/2017/"
---

Recent years have seen rapid progress in meta-learning methods, which learn (and optimize) the performance of learning methods based on data, generate new learning methods from scratch, and learn to transfer knowledge across tasks and domains. Meta-learning can be seen as the logical conclusion of the arc that machine learning has undergone in the last decade, from learning classifiers, to learning representations, and finally to learning algorithms that themselves acquire representations and classifiers. The ability to improve one’s own learning capabilities through experience can also be viewed as a hallmark of intelligent beings, and there are strong connections with work on human learning in neuroscience.

Meta-learning methods are also of substantial practical interest, since they have, e.g., been shown to yield new state-of-the-art automated machine learning methods, novel deep learning architectures, and substantially improved one-shot learning systems. 

Some of the fundamental questions that this workshop aims to address are:
- What are the fundamental differences in the learning “task” compared to traditional  “non-meta” learners?
- Is there a practical limit to the number of meta-learning layers (e.g., would a meta-meta-meta-learning algorithm be of practical use)?
- How can we design more sample-efficient meta-learning methods?
- How can we exploit our domain knowledge to effectively guide the meta-learning process?
- What are the meta-learning processes in nature (e.g, in humans), and how can we take inspiration from them?
- Which ML approaches are best suited for meta-learning, in which circumstances, and why?
- What principles can we learn from meta-learning to help us design the next generation of learning systems? 

The goal of this workshop is to bring together researchers from all the different communities and topics that fall under the umbrella of meta-learning. We expect that the presence of these different communities will result in a fruitful exchange of ideas and stimulate an open discussion about the current challenges in meta-learning, as well as possible solutions.


## Speakers ##
- [Josh Tenenbaum](http://web.mit.edu/cocosci/josh.html) (MIT)
- [Jane Wang](http://www.janexwang.com) (DeepMind)
- [Jitendra Malik](https://people.eecs.berkeley.edu/~malik/) (UC Berkeley)
- [Oriol Vinyals]() (DeepMind)
- [Chelsea Finn](https://people.eecs.berkeley.edu/~cbfinn/) (UC Berkeley) 
- [Christophe Giraud-Carrier]() (Brigham Young University)

## Additional Panelists  ##
- [Samy Bengio]() (Google)

## Organizers ##
- [Roberto Calandra](http://www.robertocalandra.com) (UC Berkeley)
- [Frank Hutter](http://www2.informatik.uni-freiburg.de/~hutter/) (University of Freiburg)
- [Hugo Larochelle](http://www.dmi.usherb.ca/~larocheh/index_en.html) (Google Brain)
- [Sergey Levine](https://people.eecs.berkeley.edu/~svlevine/) (UC Berkeley)

## Important dates ##
- Submission deadline: ~~03 November 2017~~ ([Anywhere on Earth](https://www.timeanddate.com/time/zones/aoe))
- Notification: ~~24 November 2017~~
- Camera ready: ~~04 December 2017~~
- Workshop: 09 December 2017

## Schedule ##

| --------:| ---------------------------------------------------
| 08:30 | Introduction and opening remarks 
| 08:40 | **Jitendra Malik** -- Learning to optimize with reinforcement learning
| 09:10 | [**Christophe Giraud-Carrier** -- Informing the Use of Hyperparameter Optimization Through Metalearning](slides/metalearn2017_giraud-carrier.pptx)
| 09:40	| Poster spotlights
| 10:00 | Poster session 1 ( + Coffee Break)
| 11:00 | [**Jane Wang** -- Multiple scales of task and reward-based learning](slides/metalearn2017_wang.pdf)
| 11:30 | [**Chelsea Finn** -- Model-Agnostic Meta-Learning: Universality, Inductive Bias, and Weak Supervision](slides/metalearn2017_finn.pdf)
| 12:00 | *Lunch Break*
| 13:30 | **Josh Tenenbaum** -- Learn to learn high-dimensional models from few examples
| 14:00 | Contributed talk 1: Multiple Adaptive Bayesian Linear Regression for Scalable Bayesian Optimization with Warm Start
| 14:15 | Contributed talk 2: Learning to Model the Tail 
| 14:30 | Poster session 2 ( + Coffee Break)
| 15:30 | **Oriol Vinyals** -- Meta Unsupervised Learning
| 16:00 | Panel discussion
| 17:00 | End 

<!--
## Submission instructions  ##

Papers must be in the latest NIPS format, but with a maximum of 4 pages (excluding references). Papers should include the authors (by using the \nipsfinalcopy) to your document prior to submitting). 

Accepted papers and eventual supplementary material will be made available on the workshop website. However, this does not constitute an archival publication and no formal workshop proceedings will be made available, meaning contributors are free to publish their work in archival journals or conference.

*The two best papers submitted will be presented as 15-minutes contributed talks*

**Accepted authors will be able to register to the NIPS workshops even if the registration for public is currently closed!**

Submissions can be made at [https://cmt3.research.microsoft.com/metalearn2017](https://cmt3.research.microsoft.com/metalearn2017)
-->

## Accepted Papers  ##

-  [SMASH: One-Shot Model Architecture Search through HyperNetworks](papers/metalearn17_brock.pdf)   
Andrew Brock, Theodore Lim, J.M. Ritchie, Nick Weston
- [Meta Inverse Reinforcement Learning via Maximum Reward Sharing](papers/metalearn17_li.pdf)   
Kun Li,  Joel W. Burdick
- [Learning to Learn from Weak Supervision by Full Supervision](papers/metalearn17_dehghani.pdf)   
Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, Jaap Kamps
- Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm   
Chelsea Finn, Sergey Levine
<!--[[Extended version](https://arxiv.org/pdf/1710.11622)]-->
- [Bayesian model ensembling using meta-trained recurrent neural networks](papers/metalearn17_ambrogioni.pdf)   
Luca Ambrogioni, Julia Berezutskaya, Umut Güçlü, Eva W. P. van den Borne, Yağmur Güçlütürk, Marcel A. J. van Gerven
- [Accelerating Neural Architecture Search using Performance Prediction](papers/metalearn17_baker.pdf)   
Bowen Baker, Otkrist Gupta, Ramesh Raskar, Nikhil Naik
- [Meta-Learning for Semi-Supervised Few-Shot Classification](papers/metalearn17_ren.pdf) [[Appendix](papers/metalearn17_ren_appendix.pdf)]   
Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B. Tenenbaum, Hugo Larochelle, Richard S. Zemel
- [Connectivity Learning in Multi-Branch Networks](papers/metalearn17_ahmed.pdf)   
Karim Ahmed, Lorenzo Torresani
- [A Simple Neural Attentive Meta-Learner](papers/metalearn17_mishra.pdf)   
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, Pieter Abbeel
- [Semi-Supervised Few-Shot Learning with Prototypical Networks](papers/metalearn17_boney.pdf)   
Rinu Boney, Alexander Ilin
- [Language Learning as Meta-Learning](papers/metalearn17_andreas.pdf) [[Appendix](papers/metalearn17_andreas_appendix.pdf)]    
Jacob Andreas, Dan Klein, Sergey Levine
- [Hyperparameter Optimization with Hypernets](papers/metalearn17_lorraine.pdf)      
Jonathan Lorraine, David Duvenaud
- [Few-Shot Learning with Meta Metric Learners](papers/metalearn17_cheng.pdf)   
Yu Cheng, Mo Yu, Xiaoxiao Guo, Bowen Zhou
- [Gated Fast Weights for On-The-Fly Neural Program Generation](papers/metalearn17_schlag.pdf)   
Imanol Schlag, Jürgen Schmidhuber
- [A bridge between hyperparameter optimization and learning-to-learn](papers/metalearn17_franceschi.pdf)   
Luca Franceschi, Paolo Frasconi, Michele Donini, Massimiliano Pontil
- [Understanding Short-Horizon Bias in Stochastic Meta-Optimization](papers/metalearn17_wu.pdf) [[Appendix](papers/metalearn17_wu_appendix.pdf)]   
Yuhuai Wu, Mengye Ren, Renjie Liao, Roger B. Grosse
- [Routing Networks: Adaptive Selection of Non-linear Functions for Multi-Task Learning](papers/metalearn17_rosenbaum.pdf) [[Extended version](https://arxiv.org/abs/1711.01239)]   
Clemens Rosenbaum, Tim Klinger, Matthew Riemer
- [Learning Decision Trees with Reinforcement Learning](papers/metalearn17_xiong.pdf) [[Appendix](papers/metalearn17_xiong_appendix.pdf)]   
Zheng Xiong, Wenpeng Zhang, Wenwu Zhu
- [Multiple Adaptive Bayesian Linear Regression for Scalable Bayesian Optimization with Warm Start](papers/metalearn17_perrone.pdf)   
Valerio Perrone, Rodolphe Jenatton, Matthias Seeger, Cédric Archambeau
- [Backpropagated plasticity: learning to learn with gradient descent in large plastic networks](papers/metalearn17_miconi.pdf)
Thomas Miconi, Jeff Clune, Kenneth O. Stanley
- [Learning to Learn while Learning](papers/metalearn17_kappler.pdf)   
Daniel Kappler, Stefan Schaal, Franziska Meier
- [Meta-Learning for Instance-Level Data Association](papers/metalearn17_clark.pdf)   
Ronald Clark, John McCormac, Stefan Leutenegger, Andrew J. Davison
- [Supervised Learning of Unsupervised Learning Rules](papers/metalearn17_metz.pdf)   
Luke Metz, Brian Cheung, Jascha Sohl-dickstein
- [Learning word embeddings from dictionary definitions only](papers/metalearn17_bosc.pdf)   
Tom Bosc, Pascal Vincent
- [Learning to Model the Tail](papers/metalearn17_wang.pdf) [[Extended version](https://papers.nips.cc/paper/7278-learning-to-model-the-tail)]   
Yu-Xiong Wang, Deva Ramanan, Martial Hebert
- [Born Again Neural Networks](papers/metalearn17_furlanello.pdf)   
Tommaso Furlanello, Zachary C. Lipton, Laurent Itti, Anima Anandkumar
- [Hyperactivations for Activation Function Exploration](papers/metalearn17_vercellino.pdf)   
Conner Joseph Vercellino, William Yang Wang
- [Concept Learning via Meta-Optimization with Energy Models](papers/metalearn17_mordatch.pdf)   
Igor Mordatch
- [Simple and Efficient Architecture Search for CNNs](papers/metalearn17_elsken.pdf)   
Thomas Elsken, Jan-Hendrik Metzen, Frank Hutter



## Program Committee ##

We thank the program committee for shaping the excellent technical program (in alphabetical order):   

Parminder Bhatia, Andrew Brock, Bistra Dilkina, Rocky Duan, David Duvenaud, Thomas Elsken, Dumitru Erhan, Matthias Feurer, Chelsea Finn, Roman Garnett, Christophe Giraud-Carrier, Erin Grant, Klaus Greff, Roger Grosse, Abhishek Gupta, Matt Hoffman, Aaron Klein, Marius Lindauer, Jan-Hendrik Metzen, Igor Mordatch, Randy Olson, Sachin Ravi, Horst Samulowitz, Jürgen Schmidhuber, Matthias	Seeger, Jake Snell, Jasper Snoek, Alexander	Toshev, Eleni Triantafillou, Jan van Rijn, Joaquin Vanschoren.

## Contacts  ##

For any question you can contact us at <info@metalearning.ml>

## Sponsors ##
<img src="https://upload.wikimedia.org/wikipedia/commons/2/2f/Google_2015_logo.svg" alt="googlelogo" title="google logo" height="100" />
